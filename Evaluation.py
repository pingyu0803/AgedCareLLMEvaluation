# -*- coding: utf-8 -*-
"""Evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12EYFWUhWiUYzaf2gVYSfGg0fOdime8Uv
"""

# Install required package
# !pip install bert-score pandas

#F1 Score

import pandas as pd
from bert_score import score

# 1. Load your CSV file
df1 = pd.read_csv("generated_answers.csv")  # Replace with your filename
df2 = pd.read_csv("reference_answers.csv")  # Replace with your filename

# 2. Extract answers
generated_answers = df1["generated_answers"].tolist()
reference_answers = df2["reference_answers"].tolist()

# 3. Calculate BERTScore F1
P, R, F1 = score(
    generated_answers,
    reference_answers,
    lang="en",
    model_type="bert-base-uncased",  # Default model
)

# 4. Add scores to DataFrame
df["f1"] = F1.numpy().tolist()

# 5. Save results
df.to_csv("F1.csv", index=False)


# Fairness or Bias Evaluation
# Calculate mean F1 scores for male and female
male_f1 = df_male["f1"].mean()
female_f1 = df_female["f1"].mean()

# Demographic Parity Gap
fairness_gap = male_f1 - female_f1

print("Fairness Gap:", fairness_gap)

# Robustness Evaluation
# Calculate mean F1 scores for original and noise-added data
original_f1 = df_original["f1"].mean()
noise_f1 = df_noise["f1"].mean()

# Robustness Gap
robustness_gap = original_f1 - noise_f1

print("Robustness Gap:", robustness_gap)

#ROUGE, BLEU, BLEURT, and METEOR Calculation

import pandas as pd
import nltk
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
from nltk.translate.meteor_score import meteor_score
from bleurt import score as bleurt_score

# Load data
df1 = pd.read_csv("generated_answers.csv")
df2 = pd.read_csv("reference_answers.csv")

generated_answers = df1["generated_answers"].tolist()
reference_answers = df2["reference_answers"].tolist()

# Prepare score lists
rouge1_list, rouge2_list, rougel_list = [], [], []
bleu_list, meteor_list, bleurt_list = [], [], []

# Initialize BLEURT scorer (download checkpoint once)
bleurt_checkpoint = "bleurt-base-128"
bleurt_scorer = bleurt_score.Scorer(bleurt_checkpoint)

# ROUGE scorer
rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)


# Evaluation loop
smooth_fn = SmoothingFunction().method4
for gen, ref in zip(generated_answers, reference_answers):
    # ROUGE
    scores = rouge.score(ref, gen)
    rouge1_list.append(scores['rouge1'].fmeasure)
    rouge2_list.append(scores['rouge2'].fmeasure)
    rougel_list.append(scores['rougeL'].fmeasure)

    # BLEU
    ref_tokens = nltk.word_tokenize(ref)
    gen_tokens = nltk.word_tokenize(gen)
    bleu = sentence_bleu([ref_tokens], gen_tokens, smoothing_function=smooth_fn)
    bleu_list.append(bleu)

    # METEOR
    meteor = meteor_score([ref], gen)
    meteor_list.append(meteor)

# BLEURT
bleurt_list = bleurt_scorer.score(references=reference_answers, candidates=generated_answers)

# Add all scores to df1
df1["ROUGE-1"] = rouge1_list
df1["ROUGE-2"] = rouge2_list
df1["ROUGE-L"] = rougel_list
df1["BLEU"] = bleu_list
df1["METEOR"] = meteor_list
df1["BLEURT"] = bleurt_list

# Save results
df1.to_csv("all_scores.csv", index=False)