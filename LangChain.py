# -*- coding: utf-8 -*-
"""Untitled83.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13JmmfYT_hD2rLRimECvfVz3im2rdEibS
"""

from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import DirectoryLoader, TextLoader, PyPDFLoader

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import pandas as pd  # Added for CSV support
import os

# --- 1. Load Documents ---
loader = DirectoryLoader(
    path="./RAGKnowledgeFiles",  # Path to your folder
    glob="**/*.txt",            # For PDFs: "**/*.pdf" and use loader_cls=PyPDFLoader
    loader_cls=TextLoader,       # Default for .txt files
    show_progress=True           # Shows loading progress
)
documents = loader.load()

# --- 2. Split Documents ---
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", " "]
)
docs = text_splitter.split_documents(documents)
print(f"Split into {len(docs)} chunks")

# --- 3. Embeddings ---
embedding_model = HuggingFaceEmbeddings(
    model_name="" # Embedding model
)

# --- 4. Vector Store ---
vectorstore = FAISS.from_documents(docs, embedding_model)

# --- 5. Load LLM ---
model_id = " "
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id)

llm_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.3
)
llm = HuggingFacePipeline(pipeline=llm_pipeline)

# --- 6. Setup RAG ---
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# --- 7. Load Prompt & Test Data from CSV ---
with open("prompt.txt", "r") as f:
    prompt_template = f.read().strip()

# Load test data from CSV
test_data = pd.read_csv("test_data.csv")  # Assuming CSV has 'question' column
test_questions = test_data['question'].tolist()

# --- 8. Run Queries ---
for question in test_questions:
    # Format the prompt with the question
    formatted_query = f"{prompt_template}\n\nQuestion: {question}"

    # Get the answer
    result = qa_chain({"query": formatted_query})

    # Print results
    print("Answer:", result["result"])