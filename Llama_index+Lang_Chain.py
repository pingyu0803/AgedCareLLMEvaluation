# -*- coding: utf-8 -*-
"""Untitled85.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fSdFgX2R7eo-CnVra3Lel8c0opvHki5n
"""

import torch
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader,ServiceContext
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.core.prompts.prompts import SimpleInputPrompt

# --- Load Documents ---
documents = SimpleDirectoryReader("./RAGKnowledgeFiles").load_data()  # Your documents folder

system_prompt="""
You are a Q&A assistant. Your goal is to answer questions as
accurately as possible based on the instructions and context provided.
"""

query_wrapper_prompt=SimpleInputPrompt("<|USER|>{query_str}<|ASSISTANT|>")

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
)


# Initialize the HuggingFaceLLM class with model and generation configurations
llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={"temperature": 0.0, "do_sample": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name="",
    model_name="", # Load large language model
    device_map="auto",
    model_kwargs={"torch_dtype": torch.float16 , "load_in_8bit":True}
)

from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index.core import ServiceContext
from llama_index.embeddings.langchain import LangchainEmbedding

embed_model=LangchainEmbedding(
    HuggingFaceEmbeddings(model_name=" ")) # Embedding model


# --- Setup Service Context ---
service_context = ServiceContext.from_defaults(
    llm=llm,
    embed_model=embed_model,
    chunk_size=500,
    chunk_overlap=50
)

# --- Build Vector Index ---
index = VectorStoreIndex.from_documents(
    documents,
    service_context=service_context
)

# --- Load Prompt & Test Data ---
with open("prompt.txt", "r") as f:
    prompt_template = f.read().strip()

# Load test data from CSV
test_data = pd.read_csv("test_data.csv")  # Assuming CSV has 'question' column
test_questions = test_data['question'].tolist()

# --- Query Engine ---
query_engine = index.as_query_engine(
    similarity_top_k=3,  # Retrieve top 3 chunks
    streaming=False

    # --- Run Queries ---
for question in test_questions:
    # Combine prompt template with question
    full_query = f"{prompt_template}\n\nQuestion: {question}"

    # Get response
    response = query_engine.query(full_query)
    print("Answer:", str(response))